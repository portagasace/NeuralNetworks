# NeuralNetworks
Here we have trained a multi-layer neural network by sampling 32 grayscale sub-images from the 64 image data set and making sure all values are binary (black or white). Once the images were appended into an array, we build two MLNNs: a model in heteroassociative mode (Model 1), and another in autoassociative mode (Model 2). We build three-layer neural nets for both MLNNs. We utilize the Tensorflow keras.model function that takes in a set input and applies an activation function onto the target output. We found that Model 1 is incapable of predicting images that are not included in the training set. We applied cross-validation (16 images as training set and 16 images as validation set) to tune the hyperparameters of Model 2, which help us determine the following optimal hyperparameters: 1) learning rate as 0.01; 2) the number of nodes in the hiddle layer is 288; 3) and the optimal epochs is 15.

For model 2, the activation function of the output layer is specified as Sigmoid. The Sigmoid function will return a real number between 0 to 1 on each of our 256 pixel values. The model function also reshapes the output to a 16 by 16 matrix. We then utilize the model.compile function with the optimization and loss function parameters. For the optimization function, Adam is applied which uses an estimation of the expected value and the variance (also known as the first and second moments) to adapt the learning rate for each weight in the model function.
