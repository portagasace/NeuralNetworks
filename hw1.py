# -*- coding: utf-8 -*-
"""HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V2xvp4Qh_OYbqv5PpLMGQ7d5xLTQQA29

# Step 1: Design and build the dataset

Here we load all the packages used in this project
"""

!pip install opencv-contrib-python==3.4.2.17
import tensorflow as tf
import numpy as np
from tensorflow import keras
import pandas as pd
import random
import cv2
from urllib.request import urlopen
import sys
import matplotlib.pyplot as plt
import re
from string import ascii_uppercase
from string import ascii_lowercase
import copy
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from scipy import interp
from sklearn.metrics import roc_auc_score

"""Here we create the labels for the 64 images. Note that we use 'aa' instead of 'a' to label the image 'a' because Windows cannot distinguish 'A' and 'a'."""

class_names = list(ascii_uppercase)[0:26]+[str(a) + b for a,b in zip(list(ascii_lowercase)[0:26],list(ascii_lowercase)[0:26])]+[str(i) for i in range(10)]+['colon', 'exclamatory', 'quotation']
class_labels = np.arange(65)
print(class_names)
print(class_labels)

"""In the "images" folder, we have stored 64 separate images. We use the cv2 package to resize these images and get 16 * 16 pixels. These pixels have a value between 0 to 255, and so we recode them into 0 and 1 by applying a thresholding function (127.5 is set as the cutoff value). Finally, we convert the image pixels into arrays."""

imgs = []
for img_file in class_names: 
    resp = urlopen('https://raw.githubusercontent.com/troyin/image-classification/main/images'+'/'+img_file+'.'+'gif')
    arr = np.asarray(bytearray(resp.read()), dtype="uint8")
    img_gray = cv2.imdecode(arr, cv2.IMREAD_GRAYSCALE)
    img_gray = cv2.resize(img_gray, (16, 16)) 
    (thresh, img_bw) = cv2.threshold(img_gray, 127.5, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    img = np.array(img_bw,dtype = np.float64)	
    imgs.append(img)
imgs = np.array(imgs,dtype = np.float64)/255.

"""Here, we plot the 64 images to make sure that the dataset has been loaded correctly."""

for i in range(imgs.shape[0]):
      plt.subplot(5, 13, i+1)
      plt.imshow(imgs[i],cmap='gray')
      plt.axis('off')

plt.show()

"""## Scale-Invariant Feature Transform (SIFT)

We have tried to apply the Scale-Invariant Feature Transform (SIFT) algorithms on the 64 images to find keypoints and descriptors. The SIFT algorithms allowed us to successfully identify some keypoints from some of the 64 images (e.g., ); however, for other images (e.g., ), we failed to identify any keypoints. We did not find many similar keypoints across different images. 

Given these findings, together with the observation that our images are relatively simple and contain a limited number of features, we decided not to move forward with applying the SIFT algorithms in this assignment.
"""

imgs2 = cv2.normalize(imgs, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')
features = []
none_list = []
for i in range(imgs.shape[0]):
  sift =cv2.xfeatures2d.SIFT_create()
  _, des = sift.detectAndCompute(imgs2[i], None)
  if des is None:
    none_list.append(i)
    features.append(np.zeros((1, sift.descriptorSize()), np.float32))
  else:
    features.append(des)

"""We design and build two multi-layer neural networks (MLNNs) in this assignment. One MLNN is in **heteroassociative** mode, in which we try to directly predict the labels of each image; and the other MLNN is in **autoassociative** mode, in which we try to first predict the value of each pixel in a image and then assemble these pixels to form an image.

Since we have a small sample size, we decide to build three-layer neural networks. We are concerned that building MLNNs with more layers will lead to overfitting.

We have implemented **cross-validation** in the training process. Cross-validation allows us to pick the optimal set of hyperparameters for our MLNNs. Nevertheless, we find that for Model 1, the MLNN in heteroassociative mode, a cross-validation process does not improve model performance. The main reason is that for this model, the label of images in the validation set but not in the training set can never be accurately predicted. Accordingly, we have only implemented cross-validation for Model 2, the MLNN in heteroassociation. In the cross-validation process, 16 images are assigned as the training set, and 16 images as assigned as the validation set.

We now report each MLNN separately. Note that Step 5 and 6 does not apply for Model 1 because fraction-of-hits (Fh) and fraction-of-false-alarms (Ffa) cannot be computed for it.

# Model #1: MLNN in heteroassociative mode

## Step 2: Design and develop the MLNN

***Neural network architecture***: We build the model with three layers: an input layer, an output layer, and a hidden layer between the two layers. The input layer contains 256 nodes, and the output layer has 32 nodes (32 nodes because we have 32 images to be trained). Each of the 32 nodes contains a score that indicates the current image belongs to one of the 32 classes.

1. Set up the layers. The first layer in this network, tf.keras.layers.Flatten, transforms the format of the images from a two-dimensional array (of 16 by 16 pixels) to a one-dimensional array (256 pixels). The second layer is a fully connected hidden layer that has 256 nodes and is connected with a "relu" activitation function. The output layer is specified with a "softmax" activitation here, which allows each node to return a probably indicating the likeliehood that the current image belongs to the corresponding class.
"""

model1 = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(16, 16)),
  tf.keras.layers.Dense(256, activation='relu'),
  tf.keras.layers.Dense(32, activation='softmax')
])

"""2. Compile the model. We specify a loss function that deals with multiple classification problems, an AdamOptimizer (a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments), and an accuracy metric to indicate the faction of the images that are correctly classified."""

model1.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])

"""## Step 3: Train the MLNN on the dataset

**Train the model.** There are two main considerations: 1) which 32 of the 64 images to be used as the training/validation set and which 32 of them to be used as the testing set; 2) how many epochs to train. After experimenting with differen combinations of the image sets and different numbers of epochs (100, 250, 500, 1000), we achieve the best results with the following: 1) We use A,B,D,E,G,H,I,3,5,9; 2) We use 50 epochs.
"""

model1.fit(imgs[0:32,:,:], class_labels[0:32], epochs=50)

"""## Step 4. Evaluate the MLNN

In this step, we will evaluate the predictive accuracy of the two MLNNs. We will also compute fraction-of-hits (Fh) and fraction-of-false-alarms (Ffa) for Model 2, MLNN in autoassociative mode.

**Evaluate accuracy**. The following functions show the prediction results of the model and if the predictions are accurate or now. We find that the predictions for all images that are not in the training dataset are wrong.
"""

print(np.argmax(model1.predict(imgs), axis=-1))
print(class_labels)
print(np.argmax(model1.predict(imgs), axis=-1)==class_labels)

predicted1 = imgs[np.argmax(model1.predict(imgs), axis=-1)] # prediction with noiseless images (Assignment Step 3)
for i in range(imgs.shape[0]):
      plt.subplot(5, 13, i+1)
      plt.imshow(predicted1[i],cmap='gray' )
      plt.axis('off')

plt.show()

"""
We write a function to compute the two accuracy metrics: **fraction of hits (Fh) and fraction of false alarms (Ffa)**."""

# The following function returns ffa and fh
def compute_ffa_fh(predicted_set):
  predicted=predicted_set.copy()
  ffa, fh =[], []
  for i in range(len(predicted)):
    predict, img = predicted[i].reshape(256) , imgs[i].reshape(256) # flattening 16x16 to 1x256
    correct, wrong, black_px, white_px = 0,0,0,0                    # for ffa and fh
    for j in range(len(predict)):
      predict[j] = 0 if predict[j] < 0.5 else 1                     # grayscale to binary
    black_px = np.sum(img==1)
    white_px = np.sum(img==0)
    correct = np.sum(np.logical_and(img==1, predict==1))
    wrong = np.sum(np.logical_and(img==0, predict==1))
    ffa.append(wrong/white_px)
    fh.append(correct/black_px)
  return ffa,fh

"""We graph Fh as a function of Ffa for each exemplar in the input dataset."""

# fh ~ ffa plot for the 

ffa,fh = compute_ffa_fh(predicted1)
# std_dev_list = [0]*20
ax = plt.gca()
# ax.scatter(std_dev_list,ffa,facecolors='none',edgecolors='black',)
ax.scatter(ffa,fh,color='black')
ax.set_xlabel('Ffa')
ax.set_ylabel('Fh')

"""## Step 5. Compute Receiver Operating Characteristic (ROC)

We write a function a plot the ROC curve. We can see that the MLNN has much higher predictive accuracy than the random classifier.
"""

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (Model 1)')



y_score = predicted1[32:64].flatten()
y_test = imgs[32:64].flatten()
fpr = dict()
tpr = dict()
roc_auc = dict()
fpr, tpr, _ = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)
lw = 2
rand_color = (random.random(),random.random(),random.random())
plt.plot(fpr, tpr, color=rand_color, lw=lw, label='std =%0.3f, ROC curve (area = %0.2f)' %(0,  roc_auc))
plt.legend(loc="upper left", bbox_to_anchor=(1.05, 1))


plt.show()

"""## Step 6. Perturb the dataset by adding noise

We first write a function to add noise to the images. The noise will beGaussian-distrited with 10 percent cross-section, with zero mean and a standard deviation to be specified as an input to the function.
"""

def noise_adder(imgs,dev):        
  noise = np.random.normal(0,dev,(16,16))
  indices = np.random.choice(256, replace=False, size=int(256 * 0.9))
  noise.reshape(-1)[indices]=0
  imgs_noise = copy.deepcopy(imgs)
  for i in range(imgs.shape[0]):
    imgs_noise[i,:,:] = imgs_noise[i,:,:] + noise
  imgs_noise_normalized = copy.deepcopy(imgs_noise)
  scaler = MinMaxScaler(copy=True)
  for i in range(imgs_noise.shape[0]):
    scaler.fit(imgs_noise[i])
    imgs_noise_normalized[i] = scaler.transform(imgs_noise[i])
  return imgs_noise_normalized

"""Here, we specify the standard deviation as 0.1 and visulize how the images look at when noises have been added."""

imgs_noise_normalized = noise_adder(imgs,0.1)
for i in range(imgs.shape[0]):
      plt.subplot(5, 13, i+1)
      plt.imshow(imgs_noise_normalized[i],cmap='gray' )
      plt.axis('off')

plt.show()

"""Define various gaussian noise levels"""

dev=[0.1,0.05,0.03,0.02,0.01,0.005,0.003,0.002,0.001]

"""Compute metrics fha and fh changes for each image as noise level increases"""

ax = plt.gca() # scatter plot
ax.set_xscale('log')


data =np.array([[0.00]*65]*18) #For tabulating
# print(data.shape)

predicted1_set = np.zeros((9,65,16,16), dtype=np.float32)
imgs_noisy_set = np.zeros((9,65,16,16), dtype=np.float32)
for i in range(9):
  # dev = dev/10
  imgs_noisy = noise_adder(imgs,dev[i])
  imgs_noisy_set[i]=imgs_noisy

  std_dev_list = [dev[i]]*65
  predicted1 = imgs[np.argmax(model1.predict(imgs_noisy), axis=-1)]

  predicted1_set[i]=predicted1

  ffa,fh = compute_ffa_fh(predicted1)
  data[i*2] = ffa
  data[i*2+1] = fh
  ax.scatter(std_dev_list,ffa,facecolors='none',edgecolors='black',)
  ax.scatter(std_dev_list,fh,color='black')

ax.set_ylabel('Ffa (solid) and Fh (hollow)')
ax.set_xlabel('Gaussian noise level')

"""We make a table to show how the heteroassociative MLNN respond to different levels of noise.

The results in the table suggest that for some letters, Fh and Ffa do not change with various noise levels. For some letters (mostly letters that are not in the training set), Fh and Ffa change with various noise levels. However, the relationship is not linear; that is, Fh and Fha can either increase or decrease when noise levels become smaller.
"""

row_names  = list(ascii_uppercase)[0:26]+list(ascii_lowercase)[0:26]+[str(i) for i in range(10)]+['colon', 'exclamatory', 'quotation']
column_names = []
for i in range(len(dev)):
  column_names.append('stdev -'+str(dev[i])+' Ffa')
  column_names.append('stdev -'+str(dev[i])+' Fh')
df = pd.DataFrame(data=data.T,index=row_names,columns=column_names)
df = df.style.set_properties(**{
    'font-size': '8pt',
}) 
df[1:64,]

"""Finally, we write a function to generate the ROC curves. We find that the MLNN performs the worst when the image datasets have the most noise; it performs better when the noise level is smaller."""

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (Model 1)')


def plot_roc_hang1(predicted, j):
  y_score = predicted[32:64].flatten()
  y_test = np.where(imgs_noisy_set>0.5,1,0)[j][32:64].flatten()
  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  fpr, tpr, _ = roc_curve(y_test, y_score)
  roc_auc = auc(fpr, tpr)
  lw = 2
  rand_color = (random.random(),random.random(),random.random())
  plt.plot(fpr, tpr, color=rand_color,
          lw=lw, label='std =%0.3f, ROC curve (area = %0.2f)' %(dev[j],  roc_auc))
  plt.legend(loc="upper left", bbox_to_anchor=(1.05, 1))


for k in range(0,9):
  plot_roc_hang1(predicted1_set[k], k)

plt.show()

"""# Model #2: MLNN in autoassociative mode

## Step 2: Design and develop multi-layer neural networks

Install and import the Keras Tuner, a library that helps us pick the optimal set of hyperparameters for our MLNN.
"""

!pip install -q -U keras-tuner
import kerastuner as kt

"""***Neural network architecture***: We first define the model for hypertuning. We use a model builder function here to define MLNN, which returns a compiled model. We build the model with three layers: an input layer, an output layer, and a hidden layer between the two layers. The input layer and the output layer both contain 256 nodes. The hidden layer is a dense layer whose number of units will be tuned, and its activation function is a relu function. The output layer has an sigmoid activation function.


***Hyperparameter tuning***: We tune the following hyperparameters here: 1. Number of units in the hidden dense layer; 2. The learning rate for the optimizer.
"""

def model_builder(hp):
  # Tune the number of units in the first Dense layer
  # Choose an optimal value between 32-512
  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)
  model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(16, 16)),
  tf.keras.layers.Dense(hp_units, activation='relu'),
  tf.keras.layers.Dense(256, activation='sigmoid'),
  tf.keras.layers.Reshape((16, 16))])
  # Tune the learning rate for the optimizer
  # Choose an optimal value from 0.01, 0.001, or 0.0001
  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), loss='mse')

  return model

"""We use the Hyperband tuner to perform the hypertuning and specify the following hypermodel."""

tuner = kt.Hyperband(model_builder,
                     objective='val_loss',
                     max_epochs=10,
                     factor=3,
                     directory='my_dir',
                     project_name='intro_to_kt')

"""Create a callback to stop training early after reaching a certain value for the validation loss."""

stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

"""We now run the hyperparameter search. 

We set the validation split to be 0.5 here, which means that we have split the 32 images into two subsets: **a training set of 16 images, and a validation set of 16 images.**

We find that the optimal number of units in the first densely-connected
layer is 288 and the optimal learning rate for the optimizer
is 0.01.
"""

tuner.search(imgs[0:32,:,:],imgs[0:32,:,:], epochs=50, validation_split=0.5, callbacks=[stop_early])

# Get the optimal hyperparameters
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")

"""Build the model with the optimal hyperparameters and train it on the data for 500 epochs. We record the the number of epochs that return the highest validation accuracy."""

model = tuner.hypermodel.build(best_hps)
history = model.fit(imgs[0:32,:,:], imgs[0:32,:,:], epochs=500, validation_split=0.5)

val_acc_per_epoch = [1 - x for x in history.history['val_loss']]
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
print('Best epoch: %d' % (best_epoch,))

"""## Step 3: Train the MLNN on the dataset

**Train the model.** Since the hyperparameters have been hypertuned in the previous step, we simply
"""

model2 = tuner.hypermodel.build(best_hps)

# Retrain the model
model2.fit(imgs[0:32,:,:], imgs[0:32,:,:], epochs=best_epoch)

"""## Step 4. Evaluate the MLNN and compute Fh and Ffa

We evaluate the model by **plotting the predicted 16 by 16 pixels as images** below. We can see that the letters that were not part of the train set have murky looks: their predictions are often not exactly accurate; however, the predicted shape somewhat remsembles the letter.
"""

predicted2 = model2.predict(imgs) # prediction with noiseless images (Assignment Step 3)
for i in range(imgs.shape[0]):
      plt.subplot(5, 13, i+1)
      plt.imshow(predicted2[i],cmap='gray' )
      plt.axis('off')

plt.show()

"""
We write a function to compute the two accuracy metrics: **fraction of hits (Fh) and fraction of false alarms (Ffa)**."""

# The following function returns ffa and fh
def compute_ffa_fh(predicted_set):
  predicted = predicted_set.copy()
  ffa, fh =[], []
  for i in range(len(predicted)):
    predict, img = predicted[i].reshape(256) , imgs[i].reshape(256) # flattening 16x16 to 1x256
    correct, wrong, black_px, white_px = 0,0,0,0                    # for ffa and fh
    for j in range(len(predict)):
      predict[j] = 0 if predict[j] < 0.5 else 1                     # grayscale to binary
    black_px = np.sum(img==1)
    white_px = np.sum(img==0)
    correct = np.sum(np.logical_and(img==1, predict==1))
    wrong = np.sum(np.logical_and(img==0, predict==1))
    ffa.append(wrong/white_px)
    fh.append(correct/black_px)
  return ffa,fh

"""We graph Fh as a function of Ffa for each exemplar in the input dataset."""

# fh ~ ffa plot for the 

ffa,fh = compute_ffa_fh(predicted2)
# std_dev_list = [0]*20
ax = plt.gca()
# ax.scatter(std_dev_list,ffa,facecolors='none',edgecolors='black',)
ax.scatter(ffa,fh,color='black')
ax.set_xlabel('Ffa')
ax.set_ylabel('Fh')

"""## Step 5. Compute Receiver Operating Characteristic (ROC)

We write a function to generate the ROC curve. We can see that the MLNN has much higher predictive accuracy than the random classifier.
"""

# ROC Function
def roc_graph_logan(all_imgs,all_predicted,colors):
  roc_auc = []
  pos_label = 1 #predicted values are between 0-1
  sample_weights = None
  drop_intermediate = True # Whether to drop some suboptimal thresholds which would not appear on a plotted ROC curve
  ffa_roc = dict()
  fh_roc = dict()
  thresholds = dict()
  plt.figure()
  
  for i in range(len(all_imgs)): # ravel is same as flatten or shape(-1)
    ffa_roc[i], fh_roc[i], thresholds[i] = roc_curve(all_imgs[i],all_predicted[i], pos_label, sample_weights,drop_intermediate)
    roc_auc_ = auc(ffa_roc[i], fh_roc[i])
    plt.plot(ffa_roc[i], fh_roc[i],
            label='Noise ' + str(i) + ' (area = %0.2f)' % roc_auc_,
            color=colors[i], linewidth=2)
    
  all_ffa = np.unique(np.concatenate([ffa_roc[i] for i in range(len(all_imgs))]))
 
  # Then interpolate all ROC curves at this points
  mean_fh = np.zeros_like(all_ffa)
  for i in range(len(all_imgs)):
      mean_fh += interp(all_ffa, ffa_roc[i], fh_roc[i])
  # Finally average it and compute AUC
  mean_fh /= len(all_imgs)

  roc_auc = auc(all_ffa, mean_fh) #auc - area under curve

  #average of all noise levels
  plt.plot(all_ffa, mean_fh,
          label='Average (area = %0.2f)' % roc_auc,
          color='black', linewidth=3)
  plt.xlabel('Ffa')
  plt.ylabel('Fh')
  plt.legend(loc="lower right")

predicted_no_noise = predicted[32:64].ravel() # trained on noiseless image
# get test imgs
imgs_roc =  imgs[32:64].ravel()
#noisy_imgs_roc =  imgs_noisy[32:64].ravel().astype(int)
all_imgs = [imgs_roc] # only need to add noiseless image since we show noise later
print(all_imgs[0])
all_predicted = [predicted_no_noise]
print(all_predicted[0])
colors = ['orange','lightblue'] # black is average
roc_graph(all_imgs,all_predicted,colors) # function requires set of predicted and test imgs

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (Model 2)')

y_score = predicted2[32:64].flatten()
y_test = imgs[32:64].flatten()
fpr = dict()
tpr = dict()
roc_auc = dict()
fpr, tpr, _ = roc_curve(y_test, y_score)
roc_auc= auc(fpr, tpr)
lw = 2
rand_color = (random.random(),random.random(),random.random())
plt.plot(fpr, tpr, color=rand_color,
          lw=lw, label='std =%0.3f, ROC curve (area = %0.2f)' %(0,  roc_auc))
plt.legend(loc="upper left", bbox_to_anchor=(1.05, 1))

plt.show()

"""## Step 6. Perturb the dataset by adding noise

We first write a function to add noise to the images. The noise will beGaussian-distrited with 10 percent cross-section, with zero mean and a standard deviation to be specified as an input to the function.
"""

def noise_adder(imgs,dev):        
  noise = np.random.normal(0,dev,(16,16))
  indices = np.random.choice(256, replace=False, size=int(256 * 0.9))
  noise.reshape(-1)[indices]=0
  imgs_noise = copy.deepcopy(imgs)
  for i in range(imgs.shape[0]):
    imgs_noise[i,:,:] = imgs_noise[i,:,:] + noise
  imgs_noise_normalized = copy.deepcopy(imgs_noise)
  scaler = MinMaxScaler(copy=True)
  for i in range(imgs_noise.shape[0]):
    scaler.fit(imgs_noise[i])
    imgs_noise_normalized[i] = scaler.transform(imgs_noise[i])
  return imgs_noise_normalized

"""Here, we specify the standard deviation as 0.1 and visulize how the images look at when noises have been added."""

imgs_noise_normalized = noise_adder(imgs,0.1)
for i in range(imgs.shape[0]):
      plt.subplot(5, 13, i+1)
      plt.imshow(imgs_noise_normalized[i],cmap='gray' )
      plt.axis('off')

plt.show()

"""Define various gaussian noise levels"""

dev=[0.1,0.05,0.03,0.02,0.01,0.005,0.003,0.002,0.001]

"""Compute metrics fha and fh changes for each image as noise level increases"""

ax = plt.gca() # scatter plot
ax.set_xscale('log')


data =np.array([[0.00]*65]*18) #For tabulating
# print(data.shape)

predicted2_set = np.zeros((9,65,16,16), dtype=np.float32)
imgs_noisy_set = np.zeros((9,65,16,16), dtype=np.float32)
imgs_noisy_set_roc = [] 
predicted2_set_roc = []
for i in range(9):
  # dev = dev/10
  imgs_noisy = noise_adder(imgs,dev[i])

  std_dev_list = [dev[i]]*65

  predicted2 = model2.predict(imgs_noisy) 
  predicted2_set[i]=predicted2

  predicted2_set_roc.append(predicted2[32:64].flatten()) # for logan ROC dont delete
  imgs_noisy_set[i] = imgs_noisy
  imgs_noisy = np.where(imgs_noisy>0.5,1,0)[32:64].flatten()# for logan ROC dont delete

  imgs_noisy_set_roc.append(imgs_noisy)

  ffa,fh = compute_ffa_fh(predicted2)
  data[i*2] = ffa
  data[i*2+1] = fh
  ax.scatter(std_dev_list,ffa,facecolors='none',edgecolors='black',)
  ax.scatter(std_dev_list,fh,color='black')

ax.set_ylabel('Ffa (solid) and Fh (hollow)')
ax.set_xlabel('Gaussian noise level')

"""We make a table to show how the Autoassociative MLNN responds to various noise levels.

The results in the table suggest that, as noise level decreases, Fh in general increases and Ffa in general decreases. However, these changes are more obvious when the noise levels were higher (i.e., when the gaussian noise level was close to 0.1).
"""

row_names  = list(ascii_uppercase)[0:26]+list(ascii_lowercase)[0:26]+[str(i) for i in range(10)]+['colon', 'exclamatory', 'quotation']
column_names = []
for i in range(len(dev)):
  column_names.append('stdev -'+str(dev[i])+' Ffa')
  column_names.append('stdev -'+str(dev[i])+' Fh')
df = pd.DataFrame(data=data.T,index=row_names,columns=column_names)
df = df.style.set_properties(**{
    'font-size': '8pt',
}) 
df

"""We now plot the ROC curves. The results clearly show that as noise level increases, the predictive power of the MLNN decreases. That is, the MLNN performs the best when the noise level is lowest and performs the worst when the noise level is highest."""

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (Model 2)')

for k in range(0,9):
  plot_roc_hang1(predicted2_set[k], k)

plt.show()

colors = ['orange','lightblue','darkblue','green','darkgreen','purple','teal','yellow','red']
roc_graph_logan(imgs_noisy_set_roc,predicted2_set_roc,colors) # function requires set of predicted and test imgs

"""# Step 7 Discussion

Here we have trained a multi-layer neural network by sampling 32 grayscale sub-images from the 64 image data set and making sure all values are binary (black or white).  Once the images were appended into an array, we build two MLNNs: a model in heteroassociative mode (Model 1), and another in autoassociative mode (Model 2). We build three-layer neural nets for both MLNNs. We utilize the Tensorflow keras.model function that takes in a set input and applies an activation function onto the target output. We found that Model 1 is incapable of predicting images that are not included in the training set. We applied cross-validation (16 images as training set and 16 images as validation set) to tune the hyperparameters of Model 2, which help us determine the following optimal hyperparameters: **1) learning rate as 0.01; 2) the number of nodes in the hiddle layer is 288; 3) and the optimal epochs is 15.** 

For model 2, the activation function of the output layer is specified as Sigmoid. The Sigmoid function will return a real number between 0 to 1 on each of our 256 pixel values. The model function also reshapes the output to a 16 by 16 matrix. We then utilize the model.compile function with the optimization and loss function parameters. For the optimization function, Adam is applied which uses an estimation of the expected value and the variance (also known as the first and second moments) to adapt the learning rate for each weight in the model function.


We find that cross-validation allowed us to get high-performance models much more efficiently than hand-tuning on our own. Moreover, the noise level in the image datasets affect model peformance. The model tends to perform better when the noise level is low; however, the impacts of noise level on model performance differs across the letters to be recognized from the raw image data.


When training this model, we found that the choice of the training dataset (i.e., the 32 images used for training) have a significant on the model performance. This is where the amount of epochs is inputted and the model is trained with the provided 32 images of our input. We believed that the most ideal approach to determine the optimal training set would be to develop a function that randonly selects 32 of the 64 images, train a MLNN on them, evaluate the accuracy with 32 images, and then choose the training set that has the best performance. This approach would nonetheless require a very long time to implement, and so we choose to select a subset of images that we believe to have most distinctive shapes. 

Finally, we have attempted to apply the Scale-Invariant Feature Transform (SIFT) algorithms on the 64 images to find keypoints and descriptors. The SIFT algorithms allowed us to successfully identify some keypoints from some of the 64 images but not for others. Moreover, we did not find many similar keypoints across different images. Therefore, we did not apply the SIFT algorithms in tuning the MLNNs in this assignment.
"""